{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to listen in background...\n",
      "Say something!\n",
      "Listening...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 201\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwebcam\u001b[39m\u001b[38;5;124m\"\u001b[39m, webcam_stream\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m27\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m)]:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    204\u001b[0m webcam_stream\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from threading import Lock, Thread\n",
    "\n",
    "import cv2\n",
    "import openai\n",
    "from cv2 import VideoCapture, imencode\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pyaudio import PyAudio, paInt16\n",
    "from speech_recognition import Microphone, Recognizer, UnknownValueError\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class WebcamStream:\n",
    "    def __init__(self):\n",
    "        self.stream = VideoCapture(index=0)\n",
    "        _, self.frame = self.stream.read()\n",
    "        self.running = False\n",
    "        self.lock = Lock()\n",
    "\n",
    "    def start(self):\n",
    "        if self.running:\n",
    "            return self\n",
    "\n",
    "        self.running = True\n",
    "\n",
    "        self.thread = Thread(target=self.update, args=())\n",
    "        self.thread.start()\n",
    "        return self\n",
    "\n",
    "    def update(self):\n",
    "        while self.running:\n",
    "            _, frame = self.stream.read()\n",
    "\n",
    "            self.lock.acquire()\n",
    "            self.frame = frame\n",
    "            self.lock.release()\n",
    "\n",
    "    def read(self, encode=False):\n",
    "        self.lock.acquire()\n",
    "        frame = self.frame.copy()\n",
    "        self.lock.release()\n",
    "\n",
    "        if encode:\n",
    "            _, buffer = imencode(\".jpeg\", frame)\n",
    "            return base64.b64encode(buffer)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if self.thread.is_alive():\n",
    "            self.thread.join()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        self.stream.release()\n",
    "\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, model):\n",
    "        self.chain = self._create_inference_chain(model)\n",
    "\n",
    "    def answer(self, prompt, image):\n",
    "        print(f\"Received prompt: {prompt}\")  # Print the prompt\n",
    "        if not prompt:\n",
    "            print(\"Prompt is empty or None, returning.\")\n",
    "            return\n",
    "\n",
    "\n",
    "        print(\"Prompt:\", prompt)\n",
    "\n",
    "        response = self.chain.invoke(\n",
    "            {\"prompt\": prompt, \"image_base64\": image.decode()},\n",
    "            config={\"configurable\": {\"session_id\": \"unused\"}},\n",
    "        ).strip()\n",
    "\n",
    "        print(\"Response:\", response)\n",
    "\n",
    "        if response:\n",
    "            self._tts(response)\n",
    "\n",
    "    def _tts(self, response):\n",
    "        player = PyAudio().open(format=paInt16, channels=1, rate=24000, output=True)\n",
    "\n",
    "        with openai.audio.speech.with_streaming_response.create(\n",
    "            model=\"tts-1\",\n",
    "            voice=\"alloy\",\n",
    "            response_format=\"pcm\",\n",
    "            input=response,\n",
    "        ) as stream:\n",
    "            for chunk in stream.iter_bytes(chunk_size=1024):\n",
    "                player.write(chunk)\n",
    "\n",
    "\n",
    "    # def _tts(self, response):\n",
    "    #     player = pyaudio.PyAudio().open(format=pyaudio.paInt16, channels=1, rate=24000, output=True)\n",
    "\n",
    "    #     # Create a Bark instance\n",
    "    #     bark_model = Bark()\n",
    "\n",
    "    #     # Generate audio data from the text response\n",
    "    #     audio_data = bark_model.generate_audio(response)\n",
    "\n",
    "    #     # Stream audio data in chunks\n",
    "    #     chunk_size = 1024\n",
    "    #     for i in range(0, len(audio_data), chunk_size):\n",
    "    #         player.write(audio_data[i:i + chunk_size])\n",
    "\n",
    "    #     # Stop and close the player\n",
    "    #     player.stop_stream()\n",
    "    #     player.close()\n",
    "\n",
    "\n",
    "    def _create_inference_chain(self, model):\n",
    "        SYSTEM_PROMPT = \"\"\"\n",
    "        You are a witty assistant that will use the chat history and the image \n",
    "        provided by the user to answer its questions.\n",
    "\n",
    "        Use few words on your answers. Go straight to the point. Do not use any\n",
    "        emoticons or emojis. Do not ask the user any questions.\n",
    "\n",
    "        Be friendly and helpful. Show some personality. Do not be too formal.\n",
    "        \"\"\"\n",
    "\n",
    "        prompt_template = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessage(content=SYSTEM_PROMPT),\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                (\n",
    "                    \"human\",\n",
    "                    [\n",
    "                        {\"type\": \"text\", \"text\": \"{prompt}\"},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": \"data:image/jpeg;base64,{image_base64}\",\n",
    "                        },\n",
    "                    ],\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        chain = prompt_template | model | StrOutputParser()\n",
    "\n",
    "        chat_message_history = ChatMessageHistory()\n",
    "        return RunnableWithMessageHistory(\n",
    "            chain,\n",
    "            lambda _: chat_message_history,\n",
    "            input_messages_key=\"prompt\",\n",
    "            history_messages_key=\"chat_history\",\n",
    "        )\n",
    "\n",
    "\n",
    "webcam_stream = WebcamStream().start()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
    "\n",
    "# You can use OpenAI's GPT-4o model instead of Gemini Flash\n",
    "# by uncommenting the following line:\n",
    "# model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "assistant = Assistant(model)\n",
    "\n",
    "\n",
    "def audio_callback(recognizer, audio):\n",
    "    try:\n",
    "        print(\"Processing audio...\")\n",
    "        # Try using Google instead of Whisper for testing\n",
    "        prompt = recognizer.recognize_whisper(audio, model=\"base\", language=\"english\")\n",
    "        print(f\"Recognized Prompt: {prompt}\")\n",
    "        assistant.answer(prompt, webcam_stream.read(encode=True))\n",
    "    except UnknownValueError:\n",
    "        print(\"Speech Recognition could not understand the audio.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Add debug information to confirm the callback is running\n",
    "print(\"Starting to listen in background...\")\n",
    "\n",
    "\n",
    "recognizer = Recognizer()\n",
    "microphone = Microphone()\n",
    "with microphone as source:\n",
    "    recognizer.adjust_for_ambient_noise(source)\n",
    "\n",
    "print(\"Say something!\")\n",
    "stop_listening = recognizer.listen_in_background(microphone, audio_callback)\n",
    "print('Listening...')\n",
    "\n",
    "while True:\n",
    "    cv2.imshow(\"webcam\", webcam_stream.read())\n",
    "    if cv2.waitKey(1) in [27, ord(\"q\")]:\n",
    "        break\n",
    "\n",
    "webcam_stream.stop()\n",
    "cv2.destroyAllWindows()\n",
    "stop_listening(wait_for_stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to listen in background...\n",
      "Say something!\n",
      "Listening...\n",
      "Processing audio...\n",
      "Speech Recognition could not understand the audio.\n",
      "Processing audio...\n",
      "Speech Recognition could not understand the audio.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwebcam\u001b[39m\u001b[38;5;124m\"\u001b[39m, webcam_stream\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m27\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m)]:\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     19\u001b[0m webcam_stream\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Add debug information to confirm the callback is running\n",
    "print(\"Starting to listen in background...\")\n",
    "\n",
    "\n",
    "recognizer = Recognizer()\n",
    "microphone = Microphone()\n",
    "with microphone as source:\n",
    "    recognizer.adjust_for_ambient_noise(source)\n",
    "\n",
    "print(\"Say something!\")\n",
    "stop_listening = recognizer.listen_in_background(microphone, audio_callback)\n",
    "print('Listening...')\n",
    "\n",
    "while True:\n",
    "    cv2.imshow(\"webcam\", webcam_stream.read())\n",
    "    if cv2.waitKey(1) in [27, ord(\"q\")]:\n",
    "        break\n",
    "\n",
    "webcam_stream.stop()\n",
    "cv2.destroyAllWindows()\n",
    "stop_listening(wait_for_stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to listen in background...\n",
      "Say something!\n",
      "Listening...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RAVI TEJ\\anaconda3\\envs\\assistant_venv\\Lib\\site-packages\\whisper\\__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized Prompt:  Can you listen us?\n",
      "Processing audio...\n",
      "Recognized Prompt:  Is that your name? Yes, yes. Can you listen us?\n",
      "Processing audio...\n",
      "Recognized Prompt:  Is that clear, man? Yes.\n",
      "Processing audio...\n",
      "Recognized Prompt:  Why do you think you are dumb?\n",
      "Processing audio...\n",
      "Recognized Prompt:  Yes, yes, you do think that you are dumb. You do think that you are dumb.\n",
      "Processing audio...\n",
      "Recognized Prompt:  dogare moore itani haraja teacherna chun chun khai ho maau\n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt:  I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station.\n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt:  I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station. I'm going to go to the next station.\n",
      "Processing audio...\n",
      "Recognized Prompt:  I'm going to go to the next station. I'm going to the next station. I'm going to the next station. I'm going to the next station. I'm going to the next station. I'm going to the next station. I'm going to the next station. I'm going to the next station. I'm going to the next station. I'm going to the next station. I'm going to the next station. I'm going to the next station. I'm going to the next station. I'm going to the next station.\n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt:  a hand, what you can see in my hand.\n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt: \n",
      "Processing audio...\n",
      "Recognized Prompt:  other thing I can do by utilizing this thing which is there in my hand. Can you see?\n",
      "Processing audio...\n",
      "Recognized Prompt: \n"
     ]
    }
   ],
   "source": [
    "from speech_recognition import Microphone, Recognizer, UnknownValueError\n",
    "\n",
    "\n",
    "def audio_callback(recognizer, audio):\n",
    "    try:\n",
    "        print(\"Processing audio...\")\n",
    "        # Try using Google instead of Whisper for testing\n",
    "        prompt = recognizer.recognize_whisper(audio, model=\"base\", language=\"english\")\n",
    "        print(f\"Recognized Prompt: {prompt}\")\n",
    "        # assistant.answer(prompt, webcam_stream.read(encode=True))\n",
    "    except UnknownValueError:\n",
    "        print(\"Speech Recognition could not understand the audio.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Add debug information to confirm the callback is running\n",
    "print(\"Starting to listen in background...\")\n",
    "\n",
    "\n",
    "recognizer = Recognizer()\n",
    "microphone = Microphone()\n",
    "with microphone as source:\n",
    "    recognizer.adjust_for_ambient_noise(source)\n",
    "\n",
    "print(\"Say something!\")\n",
    "stop_listening = recognizer.listen_in_background(microphone, audio_callback)\n",
    "print('Listening...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bark import SAMPLE_RATE, generate_audio, preload_models\n",
    "from scipy.io.wavfile import write as write_wav\n",
    "from IPython.display import Audio\n",
    "\n",
    "# download and load all models\n",
    "# preload_models()\n",
    "\n",
    "# generate audio from text\n",
    "text_prompt = \"\"\"\n",
    "     Hello, my name is Suno. And, uh â€” and I like pizza. [laughs] \n",
    "     But I also have other interests such as playing tic tac toe.\n",
    "\"\"\"\n",
    "audio_array = generate_audio(text_prompt)\n",
    "\n",
    "# save audio to disk\n",
    "write_wav(\"bark_generation.wav\", SAMPLE_RATE, audio_array)\n",
    "  \n",
    "# play text in notebook\n",
    "Audio(audio_array, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "from bark import generate_audio\n",
    "\n",
    "def _tts(response):\n",
    "    player = pyaudio.PyAudio().open(format=pyaudio.paInt16, channels=1, rate=24000, output=True)\n",
    "\n",
    "    audio_data = generate_audio(response)\n",
    "\n",
    "    chunk_size = 1024\n",
    "    for i in range(0, len(audio_data), chunk_size):\n",
    "        player.write(audio_data[i:i + chunk_size])\n",
    "\n",
    "    player.stop_stream()\n",
    "    player.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gtts\n",
      "  Using cached gTTS-2.5.3-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\ravi tej\\anaconda3\\envs\\assistant_venv\\lib\\site-packages (from gtts) (2.32.3)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in c:\\users\\ravi tej\\anaconda3\\envs\\assistant_venv\\lib\\site-packages (from gtts) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\ravi tej\\anaconda3\\envs\\assistant_venv\\lib\\site-packages (from click<8.2,>=7.1->gtts) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ravi tej\\anaconda3\\envs\\assistant_venv\\lib\\site-packages (from requests<3,>=2.27->gtts) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ravi tej\\anaconda3\\envs\\assistant_venv\\lib\\site-packages (from requests<3,>=2.27->gtts) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ravi tej\\anaconda3\\envs\\assistant_venv\\lib\\site-packages (from requests<3,>=2.27->gtts) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ravi tej\\anaconda3\\envs\\assistant_venv\\lib\\site-packages (from requests<3,>=2.27->gtts) (2024.7.4)\n",
      "Using cached gTTS-2.5.3-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: gtts\n",
      "Successfully installed gtts-2.5.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gtts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "language = \"en\"\n",
    "text = \"Universe and Laws of Physics are eternal. They are fundamental. They are not changing. They Don't need Creator, as the Stephan hawking said, universe is under no obilgation to make sense to us.\"\n",
    "speech = gTTS(text=text, lang=language, slow=False, tld=\"com.au\")\n",
    "speech.save('TextToSpeech.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from pyaudio import PyAudio, paInt16\n",
    "\n",
    "# Save to a bytes buffer rather than a file\n",
    "with io.BytesIO() as audio_buffer:\n",
    "    speech.write_to_fp(audio_buffer)\n",
    "    audio_buffer.seek(0)\n",
    "\n",
    "    # Open PyAudio stream for playback\n",
    "    player = PyAudio().open(format=paInt16, channels=1, rate=24000, output=True)\n",
    "\n",
    "    # Read and play audio in chunks\n",
    "    chunk_size = 1024\n",
    "    while chunk := audio_buffer.read(chunk_size):\n",
    "        player.write(chunk)\n",
    "\n",
    "    # Close the audio stream\n",
    "    player.stop_stream()\n",
    "    player.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pydub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assistant_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
